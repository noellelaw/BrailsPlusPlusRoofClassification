{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca2c7014-8385-4f36-83c1-00d72a528d5d",
   "metadata": {},
   "source": [
    "# Inferer for Hazus Earthquake Damage and Loss Analysis Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3313d40f-da35-49d2-82d0-5d2c93b7ae86",
   "metadata": {},
   "source": [
    "Written by yisangriB, fmk\n",
    "\n",
    "Dec 19, 2024\n",
    "\n",
    "This example demonstrates the final step of Brails++ workflow, which filters in the final feature set that is readily importable in R2D (or Pelicun). For this, a target regional simulation workflow needs to first be specified. In this example, we chose the **Hazus Earthquake Damage and Loss assessment** workflow. \n",
    "\n",
    "Below are six features needed to run **Hazus Earthquake (EQ) Damage and Loss assessment** workflow using R2D .\n",
    "\n",
    "| DL functions             | Feature key                               |\n",
    "|--------------------------|-------------------------------------------|\n",
    "| Building Hazus EQ Damage | StructuralType, BuildingRise, DesignLevel, FoundationType |\n",
    "| Building Hazus EQ Loss   | ReplacementCost, OccupancyClass           |\n",
    "\n",
    "\n",
    "Some features available from NSI, such as StructureType, RepairCost, NumberOfStories, YearBuilt, and OccupancyClass, can be translated to these six features, using the inferer named **Infer_features_for_HazusDL**\n",
    "\n",
    "The outcome inventory can be saved into either a geojson file or a csv file and be readily imported in R2D.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c26585-d0f5-4610-b282-e36dfc734c12",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'type' and 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     31\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrails\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Importer\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrails\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_set\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageSet    \n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbrails\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masset_inventory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Asset, AssetInventory\n",
      "File \u001b[1;32m~\\Sangri\\BrailsPlusPlus2\\examples\\inference\\../..\\brails\\__init__.py:44\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m\"\"\"Initializations and metadata for the brails package.\"\"\"\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Imports:\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Importer\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Package metadata:\u001b[39;00m\n\u001b[0;32m     47\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrails\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\Sangri\\BrailsPlusPlus2\\examples\\inference\\../..\\brails\\utils\\__init__.py:44\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) 2024 The Regents of the University of California\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# This file is part of BRAILS++.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# You should have received a copy of the BSD 3-Clause License along with\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# BRAILS. If not, see <http://www.opensource.org/licenses/>.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03mutils module of the brails package.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeo_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeoTools\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Importer\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_validator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputValidator\n",
      "File \u001b[1;32m~\\Sangri\\BrailsPlusPlus2\\examples\\inference\\../..\\brails\\utils\\geo_tools.py:61\u001b[0m\n\u001b[0;32m     57\u001b[0m R_EARTH_KM \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6371.0\u001b[39m\n\u001b[0;32m     58\u001b[0m KM2_FEET \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3280.84\u001b[39m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGeoTools\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    A collection of static methods for geospatial analysis and operations.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m            correspondence data.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhaversine_dist\u001b[39m(p1: \u001b[38;5;28mlist\u001b[39m, p2: \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n",
      "File \u001b[1;32m~\\Sangri\\BrailsPlusPlus2\\examples\\inference\\../..\\brails\\utils\\geo_tools.py:179\u001b[0m, in \u001b[0;36mGeoTools\u001b[1;34m()\u001b[0m\n\u001b[0;32m    174\u001b[0m                 rectangles\u001b[38;5;241m.\u001b[39mappend(intersection\u001b[38;5;241m.\u001b[39menvelope)\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rectangles\n\u001b[0;32m    178\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_polygon_cells\u001b[39m(bpoly: \u001b[43mPolygon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMultiPolygon\u001b[49m,\n\u001b[0;32m    180\u001b[0m                        rectangles: \u001b[38;5;28mlist\u001b[39m[Polygon],\n\u001b[0;32m    181\u001b[0m                        output_file: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    Plot a polygon and its rectangular mesh, optionally saving the plot.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m        ValueError: If `fout` is provided and an invalid filename is given.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# Plot the base polygon:\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'type' and 'type'"
     ]
    }
   ],
   "source": [
    "# Written: sy Dec 2024\n",
    "# License: BSD-2\n",
    "\n",
    "\"\"\"\n",
    " Purpose: Testing Inferer\n",
    "\"\"\"\n",
    "\n",
    "# Install packages. They are used only for the map visualization \n",
    "try:\n",
    "    import folium\n",
    "except Exception as e:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install folium\n",
    "    import folium\n",
    "\n",
    "try:\n",
    "    import plotly.express as px\n",
    "except Exception as e:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install plotly\n",
    "    import plotly.express as px\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import gc # to collect memory\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, \"../../\")\n",
    "from brails.utils import Importer\n",
    "from brails.types.image_set import ImageSet    \n",
    "from brails.types.asset_inventory import Asset, AssetInventory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dae0f1d-1a6d-4e62-a7ec-4e4413e0cdf0",
   "metadata": {},
   "source": [
    "## Step1: Set the regional boundary by its name\n",
    "\n",
    "Target city is Berkeley, CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d1435-b878-4250-bc5d-b9ac8c592385",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_data = {\"type\": \"locationName\", \"data\": \"Berkeley, CA\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce82a73-2ba4-4668-8cd7-763323289639",
   "metadata": {},
   "outputs": [],
   "source": [
    "importer = Importer()\n",
    "region_boundary_class = importer.get_class(\"RegionBoundary\")\n",
    "region_boundary_object = region_boundary_class(region_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6118886-f1e1-4a5a-a18d-4bbc08fb54a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the regional bound\n",
    "bound = region_boundary_object.get_boundary()\n",
    "bound[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9506a5-9f10-4372-b631-9cd5a132552d",
   "metadata": {},
   "source": [
    "Set the target number of possible worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b1fc7-1f51-43d8-8e4a-3fb78ef4a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pw = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5e3b8-1f98-42d0-ba8b-808287c80b1b",
   "metadata": {},
   "source": [
    "## Step2: Obtain baseline inventory data by marging OSM foot print with NSI database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c79c0d-9ae9-41dd-9155-56cbb7b98898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape NSI database\n",
    "nsi_class = importer.get_class('NSI_Parser')\n",
    "nsi = nsi_class()\n",
    "nsi_inventory = nsi.get_raw_data_given_boundary(region_boundary_object, 'ft')\n",
    "\n",
    "print('Total number of assets detected using NSI is '\n",
    "      f'{len(nsi_inventory.inventory)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f7bcea-6cf1-4d43-a330-1895cec608e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the building inventory from OSM:\n",
    "scraper_class = importer.get_class('OSM_FootprintScraper')\n",
    "scraper = scraper_class({'length': 'ft'})\n",
    "scraper_inventory = scraper.get_footprints(region_boundary_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a3a655-a809-4721-9a00-6790dec6ea84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge these footprints with the NSI raw data previously downlaoded:\n",
    "# Remeber to set \"get_extended_features\" to be true to get splitlevel and basement information - this will be used in Section 5\n",
    "new_inventory = nsi.get_filtered_data_given_inventory(scraper_inventory, \"ft\", get_extended_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dedaa61-63a9-4faf-96ea-a8481153970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Asset id=6, After Marging NSI and OSM\")\n",
    "new_inventory.get_asset_features(6)[1]  # empty or 'NA' are missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de5a251-920b-4b53-8089-5764b3a7ee5f",
   "metadata": {},
   "source": [
    "## Step 3. Run imputation to fill in missing values\n",
    "\n",
    "Imputer fills in missing values when a feature is available for a subset of buildings in a region but not for all buildings, and it imputes the values by assuming a similarity between neighboring buildings. Therefore, the assumption is that the feature is available at least for a subset of buildings, and those buildings provide reliable information of the neighboring buildings.\n",
    "\n",
    "If a feature is completely missing for the entire building stock in the area, that feature cannot be 'imputed.' Such features may be 'inferred' using external rulesets (Hazus or user-provided). See Step 5 for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4be25-9d23-48ef-aaa6-2d2c5f17393c",
   "metadata": {},
   "source": [
    "### Importing imputer and run imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c847a-f75f-4d2d-bb51-18b866c7ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "importer = Importer()\n",
    "knn_imputer_class = importer.get_class(\"KnnImputer\")\n",
    "\n",
    "imputer=knn_imputer_class(new_inventory,n_possible_worlds=n_pw,exclude_features=['lat','lon','fd_id'])\n",
    "new_inventory = imputer.impute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7cda0f-280f-4480-bbd1-3321d6d37cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Asset id=6, After imputation\")\n",
    "new_inventory.get_asset_features(6)[1]  # empty or 'NA' are missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb576d99-0a81-459a-8d48-808e092902b5",
   "metadata": {},
   "source": [
    "## Step 4. Collect Income information (Let's randomly generate it for now)\n",
    "\n",
    "At this point, Brails ++ does not automatically scrape the household income information. The user can import it from a CSV file or use a user-defined inferer to augment the information.\n",
    "For now, let's just randomly generate it from a lognormal distribution. The income information source will be integrated into brails++ in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c0947-ab8d-475d-a918-1afa5d3b017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary add incomes\n",
    "import numpy as np\n",
    "\n",
    "cal_avg = 78672 # state average\n",
    "std_dev = cal_avg*0.5 # 50% cov\n",
    "\n",
    "mean_original = cal_avg     # Mean in the original space (lognormal)\n",
    "std_dev_original = std_dev     # Standard deviation in the original space (lognormal)\n",
    "sample_size = 1         # Number of samples to generate\n",
    "# Step 1: Calculate the parameters of the underlying normal distribution\n",
    "mu = np.log(mean_original**2 / np.sqrt(std_dev_original**2 + mean_original**2))\n",
    "sigma = np.sqrt(np.log(1 + (std_dev_original**2 / mean_original**2)))\n",
    "\n",
    "# Step 2: Generate the lognormal sample using the parameters of the normal distribution\n",
    "for key, val in new_inventory.inventory.items():\n",
    "    lognormal_sample = np.random.lognormal(mean=mu, sigma=sigma, size=sample_size)\n",
    "    val.add_features({\"Income\":lognormal_sample[0]})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c88335-bf9a-4b31-ae3d-ef6870bde8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Asset id=6, After adding income\")\n",
    "new_inventory.get_asset_features(6)[1]  # empty or 'NA' are missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b770d75c-8179-4e06-9a11-738d0f4be4cb",
   "metadata": {},
   "source": [
    "## Step 5. Basic inference using NSI data\n",
    "\n",
    "Recall that below are six features that are needed to run Hazus EQ Damage and Loss assessment workflow using R2D (Pelican).\n",
    "\n",
    "| DL functions          | Feature key                               |\n",
    "|-----------------------|-------------------------------------------|\n",
    "| Building Hazus EQ Damage | StructuralType, BuildingRise, DesignLevel, FoundationType |\n",
    "| Building Hazus EQ Loss   | ReplacementCost, OccupancyClass           |\n",
    "\n",
    "\n",
    "Among the six features, \n",
    "* **StructuralType, ReplacementCost, OccupancyClass** are already provided from NSI, so we need to simply change their key names.\n",
    "* **BuildingRise, DesignLevel, FoundationType** require a simple deterministic inference, taking NumberOfStories, YearBuilt, and StructuralType as inputs\n",
    "\n",
    "The inferer module named **Infer_features_for_HazusDL** will take care of both tasks as follows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa2355-21e8-4f07-8451-cb8e9905094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing key names\n",
    "structureType_key = 'constype'\n",
    "replacementCost_key = 'repaircost'\n",
    "numberOfStories_key = 'numstories'\n",
    "yearBuilt_key = 'erabuilt'\n",
    "occupancyClass_key = 'occupancy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74079f57-dddf-44a3-933d-05fce267bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Infer_features_for_HazusDL2 = importer.get_class(\"Infer_features_for_HazusDL\")\n",
    "inferer=Infer_features_for_HazusDL2(input_inventory=new_inventory, \n",
    "                                n_possible_worlds=n_pw, \n",
    "                                yearBuilt_key = yearBuilt_key, \n",
    "                                occupancyClass_key = occupancyClass_key, \n",
    "                                numberOfStories_key = numberOfStories_key, \n",
    "                                structureType_key = structureType_key,\n",
    "                                replacementCost_key = replacementCost_key,\n",
    "                                clean_features= True)\n",
    "new_inventory_HazusDL = inferer.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a57eb-0c24-4f30-a765-edc5c558dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Asset id=6, After ruuning Inferer\")\n",
    "new_inventory_HazusDL.get_asset_features(6)[1]  # empty or 'NA' are missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c5d61-a9ad-4c47-8979-5cccc31aa76e",
   "metadata": {},
   "source": [
    "Note that the replacement cost of this building obtained from NSI is zero. If you think this is not reliable, you can also utilize Hazus inferer to improve this. See below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9833e705-475b-4152-a2a8-9b16270e7e89",
   "metadata": {},
   "source": [
    "## Step 5 (alternative). Run advanced inference using Hazus rulesets\n",
    "\n",
    "Alternativly, one can additionally estimate **StructuralType** and/or **ReplacementCost** using rulesets provided in the Hazus inventory manual 6, instead of relying on NSI dataset. This is particularly useful when user have a more reliable, high-resolution local/private data sources (e.g. for YearBuilt, Income, NumberOfStories etc.) that may provide a better information to evaluate the **StructuralType** and **ReplacementCost**. To enable this feature, provide \"new\" key names for these two features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e7844-6adc-4758-b8cb-2199e4e5fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names of NEW keys to be infered.\n",
    "structureType_key = 'StructureTypeHazus' # instead of  \"constype\" from NSI\n",
    "replacementCost_key = 'ReplacementCostHazus' # instead of \"repaircost\" from NSI\n",
    "\n",
    "# The names of existing keys to be used as \"predictors\"\n",
    "yearBuilt_key = 'erabuilt'\n",
    "occupancyClass_key = 'occupancy'\n",
    "income_key = 'Income'\n",
    "numberOfStories_key = 'numstories'\n",
    "planArea_key = 'fpAreas'\n",
    "splitLevel_key = 'splitlevel'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0257b0dd-cd61-4635-912e-32950a1804f7",
   "metadata": {},
   "source": [
    "Note that only if these keys for **StructuralType** and **ReplacementCost** don't exist in the current inventory file, the module will automatically infer it. To make the inference, additional inventory is needed, such as **Income**, **PlanArea**, and **SplitLevel**. See the user manual to understand what feature keys need to be included as predictors to infer what feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ed406-dd30-49c0-aa7a-c1c2b9b2822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Infer_features_for_HazusDL = importer.get_class(\"Infer_features_for_HazusDL\")\n",
    "inferer=Infer_features_for_HazusDL(input_inventory=new_inventory, \n",
    "                                n_possible_worlds=n_pw, \n",
    "                                yearBuilt_key = yearBuilt_key, \n",
    "                                occupancyClass_key = occupancyClass_key, \n",
    "                                numberOfStories_key = numberOfStories_key, \n",
    "                                income_key=income_key, \n",
    "                                splitLevel_key = splitLevel_key,\n",
    "                                structureType_key = structureType_key,\n",
    "                                replacementCost_key = replacementCost_key,\n",
    "                                planArea_key=planArea_key,\n",
    "                                clean_features= False)\n",
    "new_inventory_HazusDL2 = inferer.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550577fd-d56c-4587-add2-421f7bf3c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inventory_HazusDL2.get_asset_features(6)[1]  # empty or 'NA' are missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb4e44b-ab67-40a0-8f09-371c985b8254",
   "metadata": {},
   "source": [
    "Some instances of **StructuralType**  are missing in the final attributes. As instructed by the warning msg, let's do some imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85254aed-4d7b-400e-a400-155e679d109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer=knn_imputer_class(new_inventory_HazusDL2,n_possible_worlds=n_pw)\n",
    "new_inventory_HazusDL2_imputed = imputer.impute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48beb915-a463-4774-bbae-383135289d0a",
   "metadata": {},
   "source": [
    "Now let's run inferer again. Make sure to specify the name \"**StructureType**\" so that imputed values are utilized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3487595-2f7a-4225-b13f-9d486de39ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will keep StructureType we just imputed\n",
    "\n",
    "Infer_features_for_HazusDL = importer.get_class(\"Infer_features_for_HazusDL\")\n",
    "inferer=Infer_features_for_HazusDL(input_inventory=new_inventory_HazusDL2_imputed, \n",
    "                                n_possible_worlds=n_pw, \n",
    "                                yearBuilt_key = yearBuilt_key, \n",
    "                                structureType_key = 'StructureType',\n",
    "                                clean_features= True)\n",
    "new_inventory_HazusDL2 = inferer.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e0839-1a16-40fa-841a-9e4942c65db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inventory_HazusDL2.get_asset_features(6)[1]  # empty or 'NA' are missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be821f-2e38-4fe8-9856-bc18d2a884f6",
   "metadata": {},
   "source": [
    "## Step 7: Save the inventory into a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c37bf-4d0a-432a-9be2-1ce5176d1571",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = new_inventory_HazusDL2.write_to_geojson('Berkeley_building.geojson', convert_to_centroid = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc84ea-e389-4d47-835c-82893306c74d",
   "metadata": {},
   "source": [
    "## Step 8: Visualize results\n",
    "The original inventory is too big. Let's select a random subset of buildings with size of 2395 to save some memory space for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51dfc6-3e19-45c1-8c59-c461796668c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color rules - for simplicity let's group the structural type by material\n",
    "category_colors = {\n",
    "    'W1': 'blue',\n",
    "    'W2': 'blue',\n",
    "    'W': 'blue',\n",
    "    'RM1': 'green',\n",
    "    'RM2': 'green',\n",
    "    'M': 'green',\n",
    "    'C1': 'orange',\n",
    "    'C2': 'orange',\n",
    "    'C3': 'orange',\n",
    "    'C': 'orange',\n",
    "    'S1': 'black',\n",
    "    'S2': 'black',\n",
    "    'S3': 'black',\n",
    "    'S4': 'black',\n",
    "    'S5': 'black',\n",
    "    'S': 'black',\n",
    "    'PC1': 'pink',\n",
    "    'PC2': 'pink',\n",
    "    'MH': 'red',\n",
    "    'URM': 'grey',\n",
    "    'H1': 'grey',\n",
    "    'H': 'grey',\n",
    "    'nan': 'yellow',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2359e588-66f4-42a9-840a-40e0bd19c80c",
   "metadata": {},
   "source": [
    "Below are some handy visualization functions to draw polygon/scatter maps from brails inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd45e0-3dbf-4185-9675-43c5555b7e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def geo_polygon(new_inventory_simcenter, feature_key, feature_type, pw_id=0, str_colors=None, min_value=None, max_value=None, title=\"\"):\n",
    "\n",
    "    print(title)\n",
    "    new_inventory_re1=new_inventory_simcenter.get_world_realization(pw_id)\n",
    "\n",
    "    inventory_footprints = new_inventory_re1.get_coordinates()\n",
    "    geojson_data = new_inventory_re1.get_geojson()\n",
    "    # print(list(geojson_data['features'][0]['properties']))\n",
    "\n",
    "    feature_list = list(geojson_data['features'][0]['properties'])\n",
    "    \n",
    "    def get_global_centroid(list_of_polygons):\n",
    "        all_points = [point for polygon in list_of_polygons for point in polygon]\n",
    "        return (sum(x for x, _ in all_points) / len(all_points), sum(y for _, y in all_points) / len(all_points))\n",
    "\n",
    "    \n",
    "    center_tmp = get_global_centroid(inventory_footprints[0])\n",
    "    center = (center_tmp[1],center_tmp[0])\n",
    "    #center = (37.64, -122.09)\n",
    "    if feature_type==\"float\":\n",
    "        log_colormap = folium.LinearColormap(['green', 'yellow', 'red'], vmin=np.log(min_value), vmax=np.log(max_value))\n",
    "        def style_function(feature):\n",
    "            feature_val = feature['properties'].get(feature_key, min_value)  # Default to min_value if not found\n",
    "            log_feature_val = np.log(feature_val)  # Apply the logarithmic transformation\n",
    "            color = log_colormap(log_feature_val)  # Get the color from the log scale colormap\n",
    "            return {'fillColor': color, 'color': color, 'weight': 2, 'fillOpacity': 0.5}\n",
    "            \n",
    "    elif feature_type==\"str\":\n",
    "        def style_function(feature):\n",
    "            # Get the category from the feature properties (assuming the category is stored under the 'category' key)\n",
    "            category = feature['properties'].get(feature_key, 'nan')  # Default to 'nan' if no category is found\n",
    "            color = str_colors.get(category, 'yellow')\n",
    "            return {'fillColor': color, 'color':color , 'weight': 2, 'fillOpacity': 0.5}\n",
    "\n",
    "    m = folium.Map(location=center, tiles=\"cartodbpositron\", zoom_start=12)\n",
    "    #m = folium.Map(zoom_start=13)\n",
    "    g = folium.GeoJson(\n",
    "        geojson_data,\n",
    "        name=\"geojson\",\n",
    "        tooltip=folium.GeoJsonTooltip(fields=feature_list, sticky=False), # features of the first asset\n",
    "        style_function=style_function\n",
    "    ).add_to(m)\n",
    "\n",
    "    from IPython.display import display, IFrame, HTML\n",
    "\n",
    "    display(HTML(f\"\"\"\n",
    "        <div style=\"width: 600px; height: 450; border: 2px solid black; padding: 10px;\">\n",
    "            {m._repr_html_()}\n",
    "        </div>\n",
    "    \"\"\"))\n",
    "    gc.collect()\n",
    "    return \n",
    "\n",
    "def geo_scatter(new_inventory_simcenter, feature_key, feature_type, pw_id=0, str_colors=None, float_log = None, min_value=None, max_value=None, title=\"\"):\n",
    "    new_inventory_re1=new_inventory_simcenter.get_world_realization(pw_id)\n",
    "    inventory_new_df, geom_new_df, nbldg = new_inventory_re1.get_dataframe()\n",
    "    feature_list = list(inventory_new_df.columns)\n",
    "    feature_list.remove('lat')\n",
    "    feature_list.remove('lon')\n",
    "    if feature_type==\"float\":\n",
    "\n",
    "        if float_log:\n",
    "            log_values = np.log(inventory_new_df[feature_key].astype(float) + 1e-6)\n",
    "        else:\n",
    "            log_values = inventory_new_df[feature_key].astype(float)\n",
    "            \n",
    "        colorscale = [\n",
    "            [0, 'green'],   # 0 (min value)\n",
    "            [0.5, 'yellow'], # Midpoint (log scale)\n",
    "            [1, 'red']      # 1 (max value)\n",
    "        ]\n",
    "        fig = px.scatter_mapbox(inventory_new_df, \n",
    "                               lat=geom_new_df[\"Lat\"], \n",
    "                               lon=geom_new_df[\"Lon\"], \n",
    "                               color = log_values,\n",
    "                               zoom=11, \n",
    "                               mapbox_style='open-street-map',\n",
    "                               hover_data=feature_list,\n",
    "                               color_continuous_scale = colorscale,\n",
    "                               width=600, \n",
    "                               height=450,\n",
    "                               title = title)\n",
    "\n",
    "        if float_log:\n",
    "            fig.update_layout(\n",
    "                coloraxis_colorbar=dict(title=f\"logscale\",),\n",
    "                coloraxis=dict(cmin=np.log(min_value), cmax=np.log(max_value)),\n",
    "            )\n",
    "    \n",
    "    elif feature_type==\"str\":\n",
    "        fig = px.scatter_mapbox(inventory_new_df, \n",
    "                                lat=geom_new_df[\"Lat\"], \n",
    "                                lon=geom_new_df[\"Lon\"], \n",
    "                                color=inventory_new_df[feature_key].astype(str), \n",
    "                                range_color=[-1, 4], \n",
    "                                zoom=11, \n",
    "                                mapbox_style='open-street-map',\n",
    "                                color_discrete_map=str_colors,\n",
    "                                width=600, \n",
    "                                height=450,\n",
    "                                hover_data=feature_list,\n",
    "                               title = title)\n",
    "    fig.show()\n",
    "    gc.collect()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036b4d71-4044-404d-bca5-a0e8e7278efa",
   "metadata": {},
   "source": [
    "## Comparing Hazus vs SimCenter Inference \n",
    "\n",
    "Note that SimCenter inferer bases on Hazus inferer, but it additionally takes into account the three heuristic rulesets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80007f-ac6a-4b0f-970e-22a5c652d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_polygon(new_inventory_HazusDL2, 'StructureType', 'str', str_colors = category_colors, title=\"SimCenter inferer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c945913-5196-4d4b-bcbb-35b48f63a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_polygon(new_inventory_HazusDL2, 'ReplacementCost', 'float', min_value=5.e4, max_value=1.e8, title = 'Hazus replacement cost')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
